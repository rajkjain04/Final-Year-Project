{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scraping Colab  Round 5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk2AEey9OHbe"
      },
      "source": [
        "# Install Relevant Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzESAFPiN30q"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46y-trCbOzw3"
      },
      "source": [
        "# Import Relevant Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCbGPxbXOfWg"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import os\n",
        "import re \n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "import requests \n",
        "from urllib import request\n",
        "from google.colab import files\n",
        "import sys\n",
        "from selenium import webdriver"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdsSiRkEO5O_"
      },
      "source": [
        "# Import the Excel File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v7c4rTvO43S"
      },
      "source": [
        "df = pd.read_excel('List of Companies.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gN65BthPzjp"
      },
      "source": [
        "for index, row in df.iterrows():\n",
        "    sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "    chrome_options = webdriver.ChromeOptions()\n",
        "    chrome_options.add_argument('--headless')\n",
        "    chrome_options.add_argument('--no-sandbox')\n",
        "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "    company_name = row['Company Name']\n",
        "    print(company_name)\n",
        "    url = row['url']\n",
        "    print(url)\n",
        "    driver.get(url)\n",
        "    wait = WebDriverWait(driver, 20)\n",
        "    \n",
        "    try:\n",
        "      while(driver.find_element_by_xpath('//*[@id=\"load-more\"]').is_displayed()):\n",
        "        element = driver.find_element_by_xpath('//*[@id=\"load-more\"]')\n",
        "        driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
        "        button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"load-more\"]')))\n",
        "        driver.execute_script(\"arguments[0].click()\", button)\n",
        "\n",
        "    except:\n",
        "      pass \n",
        "\n",
        "    html = driver.page_source\n",
        "    f = open('{0}.html'.format(company_name), 'w')\n",
        "    f.write(html.encode('utf-8').decode('ascii', 'ignore'))\n",
        "    f.close()\n",
        "    driver.close()\n",
        "    http=urllib3.PoolManager()\n",
        "    data = open('{0}.html'.format(company_name),'r')\n",
        "    \n",
        "    soup = BeautifulSoup(data, 'html.parser')\n",
        "    data1 = soup.find('div', attrs = {'class': 'quote-page-article-listing'})\n",
        "    links = []\n",
        "    for link in data1.find_all('a'): \n",
        "      links.append(link.get('href'))\n",
        "    \n",
        "    links = list(dict.fromkeys(links))\n",
        "    newlinks = [x for x in links if not x.startswith('https')]\n",
        "    newlinks = [x for x in newlinks if not x.startswith('/earnings')]\n",
        "\n",
        "  locals()[\"df\"+str(company_name)] = pd.DataFrame(columns = ['Author', 'Date', 'Headline', 'Text'])\n",
        "\n",
        "\n",
        "  for link in newlinks: \n",
        "    try:\n",
        "      url = \"https://www.fool.com\" + link \n",
        "      html = request.urlopen(url).read().decode('utf8')\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      head = soup.find('section', attrs = {'class': 'usmf-new article-header'})\n",
        "      headline = head.find('h1').string\n",
        "      sub_headline = head.find('h2').string\n",
        "      author = soup.find('div', attrs = {'class': 'author-name'}).find('a').string\n",
        "      dates = soup.find_all('div', attrs = {'class': 'publication-date'})\n",
        "      dates_list = []\n",
        "      for date in dates: \n",
        "          dates_list.append(date.contents[2].strip())\n",
        "\n",
        "      for i in dates_list: \n",
        "          if i.startswith('Published') and len(dates_list) > 1: \n",
        "              date_final = i\n",
        "          else: \n",
        "              date_final = i \n",
        "\n",
        "      text = soup.find('span', attrs = {'class': 'article-content'})\n",
        "      paragraphs = text.find_all('p')\n",
        "      text_final = \"\"\n",
        "      for paragraph in paragraphs:\n",
        "          text_final = text_final + paragraph.getText()\n",
        "\n",
        "      text_final = text_final.replace('\\\\', '')\n",
        "      locals()[\"df2\"+str(company_name)] = pd.DataFrame({\"Author\":[author], \"Date\":[date_final], \"Headline\":[headline], \"Text\":[text_final]})\n",
        "      locals()[\"df\"+str(company_name)] = locals()[\"df\"+str(company_name)].append(locals()[\"df2\"+str(company_name)])\n",
        "\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    locals()[\"df\"+str(company_name)].to_csv(r'{0}.csv'.format(company_name))\n",
        "    files.download(\"{0}.csv\".format(company_name))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}