{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Marketwatch 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUZI0gndVbme"
      },
      "source": [
        "!pip install selenium\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeK-IVzfVUnr"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import os\n",
        "import re \n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "import time\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.common.by import By\n",
        "import urllib3\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from selenium.webdriver.common.action_chains import ActionChains\n",
        "import requests \n",
        "from urllib import request\n",
        "import sys\n",
        "from selenium import webdriver\n",
        "from google.colab import drive\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ok5056dRVlEf"
      },
      "source": [
        "df = pd.read_csv('Marketwatch List.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3dVOauEVrYt"
      },
      "source": [
        "for index, row in df.iterrows():\n",
        "  sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "\n",
        "  url = row['url']\n",
        "  company_name = row['description']\n",
        "  count = count + 1\n",
        "\n",
        "  company_names.append(company_name)\n",
        "\n",
        "  print(company_name)\n",
        "  print(url)\n",
        "  print(count)\n",
        "  print()\n",
        "\n",
        "  driver.get(url)\n",
        "  wait = WebDriverWait(driver, 20)\n",
        "  element = driver.find_element_by_xpath('/html/body/div[4]/div[6]/div[2]/div[1]/mw-tabs/div[2]/div[1]/mw-scrollable-news-v2')\n",
        "  for i in range(0, 120):\n",
        "    driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", element)\n",
        "    time.sleep(2)\n",
        "  \n",
        "  html = driver.page_source\n",
        "  driver.close()\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  data1 = soup.find('div', attrs = {'class': 'tab__pane is-active j-tabPane'})\n",
        "  links = []\n",
        "\n",
        "  for link in data1.find_all('a'): \n",
        "    links.append(link.get('href'))\n",
        "  \n",
        "  links = list(dict.fromkeys(links))\n",
        "  links = [link for link in links if link is not None]\n",
        "  newlinks = [] \n",
        "  for link in links: \n",
        "    if link.startswith('https://www.marketwatch.com/story') or link.startswith('https://www.marketwatch.com/article'):\n",
        "      newlinks.append(link) \n",
        "    else:\n",
        "      continue\n",
        "  \n",
        "  locals()[\"df\"+str(company_name)] = pd.DataFrame(columns = ['Date', 'Headline', 'Text'])\n",
        "  for link in newlinks:\n",
        "    try: \n",
        "      url = link \n",
        "      html = request.urlopen(url).read().decode('utf-8')\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "      \n",
        "      head = soup.find('div', attrs = {'class': 'article__masthead'})\n",
        "      headline = head.find('h1').string \n",
        "      \n",
        "      if headline == None: \n",
        "        head = soup.find('div', attrs = {'class': 'article__masthead'})\n",
        "        headline = head.find('h1', attrs = {'class': 'article__headline'})\n",
        "        headline = headline.contents[2]\n",
        "      \n",
        "      date = head.find('time', attrs = {'class': 'timestamp timestamp--pub'}).string\n",
        "      text = soup.find('div', attrs = {'id': 'js-article__body'})\n",
        "      \n",
        "      if text == None: \n",
        "        continue \n",
        "      \n",
        "      paragraphs = text.find_all('p')\n",
        "      \n",
        "      text_final = \"\"\n",
        "      for paragraph in paragraphs: \n",
        "        text_final = text_final + paragraph.getText()\n",
        "\n",
        "      date = \" \".join(date.split())\n",
        "      headline = \" \".join(headline.split())\n",
        "      text_final = \" \".join(text_final.split())\n",
        "      locals()[\"df2\"+str(company_name)] = pd.DataFrame({\"Date\":[date], \"Headline\":[headline], \"Text\":[text_final]})\n",
        "      locals()[\"df\"+str(company_name)] = locals()[\"df\"+str(company_name)].append(locals()[\"df2\"+str(company_name)])\n",
        "    \n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "\n",
        "  locals()[\"df\"+str(company_name)].to_csv('{0}.csv'.format(company_name))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}